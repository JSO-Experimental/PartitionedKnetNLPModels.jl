var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [PartitionedKnetNLPModels]","category":"page"},{"location":"reference/#NLPModels.grad!-Union{Tuple{C}, Tuple{S}, Tuple{T}, Tuple{PartitionedKnetNLPModel{T, S, C, Y} where Y<:PartitionedStructures.M_part_mat.Part_mat{T}, AbstractVector{T}, AbstractVector{T}}} where {T, S, C}","page":"Reference","title":"NLPModels.grad!","text":"\tg = grad!(nlp, x, g)\n\nEvaluate ∇f(x), the gradient of the objective function at x in place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.obj-Union{Tuple{C}, Tuple{S}, Tuple{T}, Tuple{PartitionedKnetNLPModel{T, S, C, Y} where Y<:PartitionedStructures.M_part_mat.Part_mat{T}, AbstractVector{T}}} where {T, S, C}","page":"Reference","title":"NLPModels.obj","text":"\tf = obj(nlp, x)\n\nEvaluate `f(x)`, the objective function of `nlp` at `x`.\n\n\n\n\n\n","category":"method"},{"location":"reference/#PartitionedKnetNLPModels.PS_deduction-Tuple{T} where T<:KnetNLPModels.Chain","page":"Reference","title":"PartitionedKnetNLPModels.PS_deduction","text":"PS_deduction(c,dp)\n\nCompute the partially separable structure of a network represented by the Chain c by performing a forward evaluation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#PartitionedKnetNLPModels.precompile_ps_struct-Tuple{T} where T<:KnetNLPModels.Chain","page":"Reference","title":"PartitionedKnetNLPModels.precompile_ps_struct","text":"precompile_ps_struct(network<:Chain)\n\nThe function is called on a network defined by Dense/Seplayer/Conv or an other layer that define also psstruct(layer); index. The result is a precompiled PS structure. Il faut déterminer pour chaque couche une fonction ps_struct qui déterminer l'information nécessaire à prendre en compte pour déterminer la séparabilité partielle du réseau. Sa forme est très proche de celle du réseau initial, mais chaque arc (ie variable) contient l'indice de la variable. Attention il ne faut pas oublier de stocker les biais qui sont également des variables. L'objectif est d'ensuite appelé une fonction évaluant rapidement la structure PS du réseau., par propagation des dépendances sur une évaluation backward. Pour une analyse de la structure sous influence du dropout j'ai intégré des vecteur booléen indiquant neurones de la couche précédente étaient actifs ou non. Cela se modélise par le vecteur dp dans PsDense\n\n\n\n\n\n","category":"method"},{"location":"reference/#PartitionedKnetNLPModels.ps-Tuple{PartitionedKnetNLPModels.PsDense, Vector{Vector{Int64}}}","page":"Reference","title":"PartitionedKnetNLPModels.ps","text":"ps(psd::PsDense, Dep::Vector{Vector{int}})\n\npour chaque noeud on va ajouter les dépendances du layer propre (excepté dropout)  on y ajoute ensuite les dépendances (car dense) de la couche précédente modélisé par Di (à modérer avec le dropout) Le cas de la couche Dense est particulier, sans dropout il n'a que peu d'intérêt\n\n\n\n\n\n","category":"method"},{"location":"reference/#PartitionedKnetNLPModels.ps-Tuple{PartitionedKnetNLPModels.PsSep, Vector{Vector{Int64}}}","page":"Reference","title":"PartitionedKnetNLPModels.ps","text":"ps(pss::PsSep, Dep::Vector{Vector{int}})\n\npour chaque noeud on va ajouter les dépendances du layer propre: une sous partie de la couche précédente (+ dropout)  on y ajoute ensuite certaines dépendances de la couche précédente modélisé par Di (à modérer avec le dropout) Dans le cas PS, la propagation des variables sera moins intense que le cas Dense\n\n\n\n\n\n","category":"method"},{"location":"#PartitionedKnetNLPModels.jl","page":"Home","title":"PartitionedKnetNLPModels.jl","text":"","category":"section"},{"location":"tutorial/#PartitionedKnetNLPModels.jl-Tutorial","page":"Tutorial","title":"PartitionedKnetNLPModels.jl Tutorial","text":"","category":"section"}]
}
